{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### CSC 219 Machine Learning (Fall 2023)\n",
    "\n",
    "#### Dr. Haiquan Chen, Dept of Computer Scicence\n",
    "\n",
    "#### California State University, Sacramento\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3JhHIQqNe4Qs"
   },
   "source": [
    "<a name='section0'></a>\n",
    "# Lab 6:  Keras Tuner:  Automatic Hyperparameter Tuning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jyTtW4rCHHSi"
   },
   "source": [
    "There are several libraries developed for tuning the hyperparameters of neural networks. One is the ***Keras Tuner*** for tuning Keras models. \n",
    "\n",
    "The Keras Tuner is somewhat similar to the Grid Search and Random Search in scikit-learn, and allows to define the search space for the hyperparameters over which the model will be fit, and it returns an optimal set of hyperparameters. \n",
    "\n",
    "Keras Tuner is not part of the `Keras` package and it needs to be installed and imported. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 9714,
     "status": "ok",
     "timestamp": 1681493301570,
     "user": {
      "displayName": "Victor Chen",
      "userId": "00864070257040861597"
     },
     "user_tz": 420
    },
    "id": "0rnI5QO4MN0k",
    "outputId": "7c0c499d-ae0a-4bad-e9d3-9efd8f60f1d7"
   },
   "outputs": [],
   "source": [
    "#!pip install -q -U keras-tuner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "executionInfo": {
     "elapsed": 5709,
     "status": "ok",
     "timestamp": 1681493313732,
     "user": {
      "displayName": "Victor Chen",
      "userId": "00864070257040861597"
     },
     "user_tz": 420
    },
    "id": "5XryNaEBZCAu"
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "import keras_tuner as kt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "bDOJpYwzZGZP"
   },
   "source": [
    "### Load MNIST Dataset\n",
    "\n",
    "To demonstrate the use of the Keras Tuner we will work with the MNIST dataset. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 3598,
     "status": "ok",
     "timestamp": 1681493334987,
     "user": {
      "displayName": "Victor Chen",
      "userId": "00864070257040861597"
     },
     "user_tz": 420
    },
    "id": "sLU9M0eRZCC_",
    "outputId": "ec1763e7-f8ad-4bbc-9a70-8f2e9ff48c2a"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-labels-idx1-ubyte.gz\n",
      "29515/29515 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/train-images-idx3-ubyte.gz\n",
      "26421880/26421880 [==============================] - 2s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-labels-idx1-ubyte.gz\n",
      "5148/5148 [==============================] - 0s 0us/step\n",
      "Downloading data from https://storage.googleapis.com/tensorflow/tf-keras-datasets/t10k-images-idx3-ubyte.gz\n",
      "4422102/4422102 [==============================] - 0s 0us/step\n"
     ]
    }
   ],
   "source": [
    "(img_train, label_train), (img_test, label_test) = tf.keras.datasets.fashion_mnist.load_data()\n",
    "\n",
    "\n",
    "img_train = img_train.reshape(img_train.shape[0], 28, 28, 1)\n",
    "img_test = img_test.reshape(img_test.shape[0], 28, 28, 1)\n",
    "\n",
    "\n",
    "# Normalize pixel values between 0 and 1\n",
    "img_train = img_train.astype('float32') / 255.0\n",
    "img_test = img_test.astype('float32') / 255.0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "num_classes = 10\n",
    "\n",
    "# Converts a class vector (integers) to binary class matrix.   One-hot encoding!  Use with categorical_crossentropy.\n",
    "label_train = tf.keras.utils.to_categorical(label_train, num_classes)\n",
    "label_test = tf.keras.utils.to_categorical(label_test, num_classes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8yIFg-EBJQ9b"
   },
   "source": [
    "### Model Builder\n",
    "\n",
    "In the cell below, a function called `model_builder` is created, which performs search over two hyperparameters:\n",
    "\n",
    "- Number of neurons in the first Dense layer,\n",
    "- Learning rate. \n",
    "\n",
    "The line `hp_units = hp.Int('units', min_value=32, max_value=512, step=32)` defines a grid search for the number of neurons in the Dense layer in the range [32, 64, 96, ..., 512].\n",
    "\n",
    "Next, a grid search for the learning rate is defined in the range `[1e-2, 1e-3, 1e-4]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "executionInfo": {
     "elapsed": 276,
     "status": "ok",
     "timestamp": 1681493503472,
     "user": {
      "displayName": "Victor Chen",
      "userId": "00864070257040861597"
     },
     "user_tz": 420
    },
    "id": "w-A-8qDWZCFg"
   },
   "outputs": [],
   "source": [
    "def model_builder(hp):\n",
    "  model = tf.keras.Sequential()\n",
    "  \n",
    "  model.add(tf.keras.layers.Conv2D(64, kernel_size=(3, 3), strides=(1, 1),\n",
    "                 activation='relu',\n",
    "                 input_shape=(28, 28, 1)))\n",
    "  \n",
    "  model.add(tf.keras.layers.Flatten())\n",
    "\n",
    "  # Tune the number of units in the first Dense layer\n",
    "  # Choose an optimal value between 32-512\n",
    "  hp_units = hp.Int('units', min_value=32, max_value=512, step=32)\n",
    "  model.add(tf.keras.layers.Dense(units=hp_units, activation='relu'))\n",
    "  model.add(tf.keras.layers.Dense(10, activation='softmax'))\n",
    "\n",
    "  # Tune the learning rate for the optimizer\n",
    "  # Choose an optimal value from 0.01, 0.001, or 0.0001\n",
    "  hp_learning_rate = hp.Choice('learning_rate', values=[1e-2, 1e-3, 1e-4])\n",
    "\n",
    "  model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=hp_learning_rate),\n",
    "                loss=tf.keras.losses.categorical_crossentropy,\n",
    "                metrics=['accuracy'])\n",
    "\n",
    "  return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "E49wGjJDLdtS"
   },
   "source": [
    "### Hyperparameter Tuning\n",
    "\n",
    "The Keras Tuner has four tuning algorithms available: \n",
    "\n",
    "- RandomSearch Tuner, similar to the Random Grid in scikit-learn performs a random search over a distribution of values for the hyperparameters.\n",
    "- Hyperband Tuner, trains a large number of models for a few epochs and carries forward only the top-performing half of models to the next round, to converge to a high-performing model. \n",
    "- BayesianOptimization Tuner, performs BayesianOptimization by creating a probabilistic mapping of the model to the loss function, and iteratively evaluating promising sets of hyperparameters.\n",
    "- Sklearn Tuner, designed for use with scikit-learn models.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "executionInfo": {
     "elapsed": 306,
     "status": "ok",
     "timestamp": 1681495199560,
     "user": {
      "displayName": "Victor Chen",
      "userId": "00864070257040861597"
     },
     "user_tz": 420
    },
    "id": "BErSgwT8ZCHH"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'kt' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[12], line 9\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m#tuner = kt.Hyperband(model_builder,\u001b[39;00m\n\u001b[1;32m      2\u001b[0m                  \u001b[38;5;66;03m#    objective='val_accuracy',\u001b[39;00m\n\u001b[1;32m      3\u001b[0m                  \u001b[38;5;66;03m#    max_epochs=10,\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;66;03m#max_trials represents the number of hyperparameter combinations that will be tested by the tuner, \u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;66;03m#while execution_per_trial is the number of models that should be built and fit for each trial for robustness purposes.\u001b[39;00m\n\u001b[0;32m----> 9\u001b[0m tuner \u001b[38;5;241m=\u001b[39m kt\u001b[38;5;241m.\u001b[39mBayesianOptimization(\n\u001b[1;32m     10\u001b[0m     model_builder,\n\u001b[1;32m     11\u001b[0m     objective\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mval_accuracy\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     12\u001b[0m     max_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m2\u001b[39m,\n\u001b[1;32m     13\u001b[0m     executions_per_trial\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m,\n\u001b[1;32m     14\u001b[0m     directory\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmnist_kt_test\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     15\u001b[0m     overwrite\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m     16\u001b[0m )\n",
      "\u001b[0;31mNameError\u001b[0m: name 'kt' is not defined"
     ]
    }
   ],
   "source": [
    "#tuner = kt.Hyperband(model_builder,\n",
    "                 #    objective='val_accuracy',\n",
    "                 #    max_epochs=10,\n",
    "                 #    factor=3)\n",
    "\n",
    "#max_trials represents the number of hyperparameter combinations that will be tested by the tuner, \n",
    "#while execution_per_trial is the number of models that should be built and fit for each trial for robustness purposes.\n",
    "\n",
    "tuner = kt.BayesianOptimization(\n",
    "    model_builder,\n",
    "    objective=\"val_accuracy\",\n",
    "    max_trials=2,\n",
    "    executions_per_trial=1,\n",
    "    directory=\"mnist_kt_test\",\n",
    "    overwrite=True,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The following code take about 1 min for each trial for the ENTIRE dataset\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 127043,
     "status": "ok",
     "timestamp": 1681495339810,
     "user": {
      "displayName": "Victor Chen",
      "userId": "00864070257040861597"
     },
     "user_tz": 420
    },
    "id": "ZFuW74JEZCJu",
    "outputId": "65838729-1004-4553-b139-7f8131b5b8a9"
   },
   "outputs": [],
   "source": [
    "tuner.search(img_train[0:1000], label_train[0:1000], epochs=2, validation_data=(img_test[0:1000], label_test[0:1000]), callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 308,
     "status": "ok",
     "timestamp": 1681495350906,
     "user": {
      "displayName": "Victor Chen",
      "userId": "00864070257040861597"
     },
     "user_tz": 420
    },
    "id": "ido1kjnnMrgt",
    "outputId": "5485744e-e2fa-469f-ec7a-8c930c7b23d6"
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'tuner' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Get the optimal hyperparameters\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m best_hps \u001b[38;5;241m=\u001b[39m tuner\u001b[38;5;241m.\u001b[39mget_best_hyperparameters(num_trials\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m)[\u001b[38;5;241m0\u001b[39m]          \u001b[38;5;66;03m# num_trials: Optional number of HyperParameters objects to return.\u001b[39;00m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimal number of neuron in the Dense layer: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_hps\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124munits\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28mprint\u001b[39m (\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mOptimal learning rate: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mbest_hps\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mlearning_rate\u001b[39m\u001b[38;5;124m'\u001b[39m)\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tuner' is not defined"
     ]
    }
   ],
   "source": [
    "# Get the optimal hyperparameters\n",
    "best_hps = tuner.get_best_hyperparameters(num_trials=1)[0]          # num_trials: Optional number of HyperParameters objects to return.\n",
    "\n",
    "print(f\"Optimal number of neuron in the Dense layer: {best_hps.get('units')}\")\n",
    "print (f\"Optimal learning rate: {best_hps.get('learning_rate')}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zr7kuBHzN22Y"
   },
   "source": [
    "### Train and Evaluate the Model\n",
    "\n",
    "Next, we will use the optimal hyperparameters from the Keras Tuner to create a model, and afterward we will evaluate the accuracy on the test dataset. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 10985,
     "status": "ok",
     "timestamp": 1681495393122,
     "user": {
      "displayName": "Victor Chen",
      "userId": "00864070257040861597"
     },
     "user_tz": 420
    },
    "id": "IMzOKPBSZgv1",
    "outputId": "c475a672-2d0e-4cb2-8072-6ae080b43360"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/2\n",
      "32/32 - 4s - loss: 3.0903 - accuracy: 0.6050 - val_loss: 0.8005 - val_accuracy: 0.7130\n",
      "Epoch 2/2\n",
      "32/32 - 3s - loss: 0.4335 - accuracy: 0.8470 - val_loss: 0.6588 - val_accuracy: 0.7920\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<tensorflow.python.keras.callbacks.History at 0x1789343a820>"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Build the model with the optimal hyperparameters and train it on the data for 2 epochs\n",
    "model = tuner.hypermodel.build(best_hps)\n",
    "model.fit(img_train[0:1000], label_train[0:1000], epochs=2, validation_data=(img_test[0:1000], label_test[0:1000]), verbose=2, callbacks=[tf.keras.callbacks.EarlyStopping(monitor='val_loss', patience=5)])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "executionInfo": {
     "elapsed": 909,
     "status": "ok",
     "timestamp": 1681495429947,
     "user": {
      "displayName": "Victor Chen",
      "userId": "00864070257040861597"
     },
     "user_tz": 420
    },
    "id": "X-7cpcnG6S3f",
    "outputId": "42669f2f-5fb4-4330-f459-3a3c2b95cd50"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "32/32 [==============================] - 0s 11ms/step - loss: 0.6588 - accuracy: 0.7920\n",
      "[val loss, val accuracy]: [0.6587802767753601, 0.7919999957084656]\n"
     ]
    }
   ],
   "source": [
    "eval_result = model.evaluate(img_test[0:1000], label_test[0:1000])\n",
    "print(\"[val loss, val accuracy]:\", eval_result)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "machine_shape": "hm",
   "provenance": [
    {
     "file_id": "https://github.com/avakanski/Fall-2022-Python-Programming-for-Data-Science/blob/main/Lectures/Theme%203%20-%20Model%20Engineering%20Pipelines/Lecture%2021%20-%20Model%20Selection%2C%20Hyperparameter%20Tuning/Lecture%2021%20-%20Model%20Selection%2C%20Hyperparameter%20Tuning.ipynb",
     "timestamp": 1681492116891
    }
   ]
  },
  "gpuClass": "premium",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
